{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation for Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../core/database')\n",
    "import config\n",
    "from sqlalchemy import create_engine\n",
    "import itertools\n",
    "from geopy.distance import geodesic as gd\n",
    "import copy\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from typing import List,Callable,Tuple\n",
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_dotenv()\n",
    "db = os.getenv('db')\n",
    "user = os.getenv('user')\n",
    "password = getpass('Password : ')\n",
    "host = os.getenv('host')\n",
    "port = os.getenv('port')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLALCHEMY_DB_URL = config.settings.POSTGRES_URL\n",
    "engine = create_engine(SQLALCHEMY_DB_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_port(param):\n",
    "    sql = 'select * FROM rms_pelni.master_port'\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    df_port = df[df[\"is_deleted\"] == False]\n",
    "    # df_port = df_port[df_port['latitude'].isna()==False]\n",
    "    # df_port = df_port[df_port['longitude'].isna()==False]\n",
    "    df_port = df_port[df_port['flag_ppss'] == 'Y']\n",
    "    df_port = df_port.drop_duplicates()\n",
    "    df_port = df_port[['id_port', 'code', 'port_code', 'port_name', 'flag_pioneer', \n",
    "                  'height_at_high_tide', 'height_at_low_tide', 'wharf_depth',\n",
    "                  'wharf_length', 'latitude', 'longitude', 'timezone_offset',\n",
    "                  'max_berth_time', 'min_berth_time', 'number_of_berth', 'avg_berth_time']]\n",
    "    df_port = df_port.rename(columns={'id_port' : 'id', 'port_code' : 'portcode', \n",
    "                                      'port_name' : 'name', 'flag_pioneer' : 'homebase', \n",
    "                                      'height_at_high_tide' : 'hightide', \n",
    "                                      'height_at_low_tide' : 'lowtide', \n",
    "                                      'wharf_depth' : 'depth', \n",
    "                                      'wharf_length' : 'length', \n",
    "                                      'latitude' : 'lat', 'longitude' : 'long', \n",
    "                                      'timezone_offset' : 'region', \n",
    "                                      'max_berth_time' : 'maxberthtime', \n",
    "                                      'min_berth_time' : 'minberthtime', \n",
    "                                      'number_of_berth' : 'noberth', \n",
    "                                      'avg_berth_time':'avgberth'})\n",
    "    df_port.insert(0, 'portno', df_port.index)\n",
    "    set_port = df_port['code']\n",
    "\n",
    "    if param == ('001'):\n",
    "        return df_port\n",
    "    elif param == ('002'):\n",
    "        return set_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cleaned_port(df_port):\n",
    "#     port = copy.deepcopy(df_port)\n",
    "    \n",
    "#     port['depth'] = port['depth'].astype(str).str.extract('(\\d+)')\n",
    "#     port.loc[((port['depth'] == '') | (port['depth'] == None) | (port['depth'].isna())), 'depth'] = 10\n",
    "#     port['depth'] = port['depth'].astype(float)\n",
    "\n",
    "#     port['length'] = port['length'].astype(str).str.extract('(\\d+)')\n",
    "#     port.loc[((port['length'] == '') | (port['length'] == None) | (port['length'].isna())), 'length'] = 200\n",
    "#     port['length'] = port['length'].astype(float)\n",
    "\n",
    "#     port['avgberth'] = port['avgberth'].astype(str).str.extract('(\\d+)').astype(float)\n",
    "#     port.loc[((port['avgberth'] == '') | (port['avgberth'] == None) | (port['avgberth'].isna())), 'avgberth'] = 2\n",
    "    \n",
    "#     port['lat'] = port['lat'].astype(str).str.extract('(\\d+)')\n",
    "#     port.loc[((port['lat'] == '') | (port['lat'] == None) | (port['lat'].isna())), 'lat'] = 24\n",
    "#     port['lat'] = port['lat'].astype(float)\n",
    "\n",
    "#     port['long'] = port['long'].astype(str).str.extract('(\\d+)')\n",
    "#     port.loc[((port['long'] == '') | (port['long'] == None) | (port['long'].isna())), 'long'] = (-96)\n",
    "#     port['long'] = port['long'].astype(float)\n",
    "    \n",
    "#     port['noberth'] = port['noberth'].astype(str).str.extract('(\\d+)')\n",
    "#     port.loc[((port['noberth'] == '') | (port['noberth'] == None) | (port['noberth'].isna())), 'noberth'] = 1\n",
    "#     port['noberth'] = port['noberth'].astype(int)\n",
    "\n",
    "#     minberthtime = '00:00:00'\n",
    "#     minberthtime = datetime.strptime(minberthtime, '%H:%M:%S')\n",
    "#     port.loc[((port['minberthtime'] == 0) | (port['minberthtime'] == None) | (port['minberthtime'].isna())), 'minberthtime'] = minberthtime\n",
    "#     port['minberthtime'] = pd.to_datetime(port['minberthtime'])\n",
    "#     port['minberthtime'] = port['minberthtime'].dt.time\n",
    "        \n",
    "#     maxberthtime = '23:59:59'\n",
    "#     maxberthtime = datetime.strptime(maxberthtime, '%H:%M:%S')\n",
    "#     port.loc[((port['maxberthtime'] == 0) | (port['maxberthtime'] == None) | (port['maxberthtime'].isna())), 'maxberthtime'] = maxberthtime\n",
    "#     port['maxberthtime'] = pd.to_datetime(port['maxberthtime'])\n",
    "#     port['maxberthtime'] = port['maxberthtime'].dt.time\n",
    "\n",
    "#     return port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_ship(param):\n",
    "    sql = 'select * FROM rms_pelni.master_ship'\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    df_ship = df[df[\"is_deleted\"] == False]\n",
    "    df_ship = df_ship.drop_duplicates()\n",
    "    df_ship = df_ship[['id_ship', 'ship_code', 'name', 'speed', 'fuel_need',\n",
    "                       'capacity_pax', 'capacity_cargo', 'capacity_container_dry',\n",
    "                       'capacity_container_reefer', 'capacity_vehicle_truck', \n",
    "                       'capacity_vehicle_car', 'capacity_vehicle_motorcycle', \n",
    "                       'capacity_redpack', 'length_overal','draught']]\n",
    "    df_ship = df_ship.rename(columns={'id_ship' : 'id', 'ship_code' : 'code', \n",
    "                                      'fuel_need' : 'fuel', \n",
    "                                      'capacity_pax' : 'capacitypax', \n",
    "                                      'capacity_cargo' : 'capacitycargo', \n",
    "                                      'capacity_container_dry' : 'capacitydry', \n",
    "                                      'capacity_container_reefer' : 'capacityreefer', \n",
    "                                      'capacity_vehicle_truck' : 'capacitytruck', \n",
    "                                      'capacity_vehicle_car' : 'capacitycar', \n",
    "                                      'capacity_vehicle_motorcycle' : 'capacitymotor', \n",
    "                                      'capacity_redpack' : 'capacityredpack', \n",
    "                                      'length_overal' : 'length',\n",
    "                                      'draught':'draft'})\n",
    "    df_ship.insert(0, 'shipno', df_ship.index)\n",
    "    set_ship = df_ship['code']\n",
    "\n",
    "    if param == ('001'):\n",
    "        return df_ship\n",
    "    elif param == ('002'):\n",
    "        return set_ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_portdistance():\n",
    "    sql = 'select * FROM rms_pelni.master_port_distance'\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    df_portdistance = df[df['is_deleted'] ==  False]\n",
    "    df_portdistance = df_portdistance.drop_duplicates()\n",
    "    df_portdistance = df_portdistance[['code_origin', 'code_destination', 'nautical', 'commercial', \n",
    "                                       'port_origin', 'port_destination']]\n",
    "    df_portdistance = df_portdistance.rename(columns={'code_origin' : 'origin', \n",
    "                                                      'code_destination' : 'destination', \n",
    "                                                      'port_origin' : 'id_origin', \n",
    "                                                      'port_destination' :'id_destination'})\n",
    "    return df_portdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_revenue(typerev):\n",
    "    sql =  f'select * from rms_pelni.hist_pax_revenue hpr where type_rev = {typerev}'\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    #df_revenue = df[df['type_rev'] == type_rev]\n",
    "    df_revenue = df[df['is_deleted'] == False]\n",
    "    df_revenue = df_revenue.drop_duplicates()\n",
    "    df_revenue = df_revenue[['origin_port', 'destination_port', 'departure_date',\n",
    "                             'departure', 'revenue', 'total', 'type']]\n",
    "    df_revenue = df_revenue.rename(columns={'origin_port' : 'origin', \n",
    "                                            'destination_port' : 'destination', \n",
    "                                            'departure_date' : 'depdate', \n",
    "                                            'departure' : 'deptime'})\n",
    "    return df_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_maintenance():\n",
    "    sql = 'select * FROM rms_pelni.master_ship_maintenance'\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    df_maintenance = df[df['is_deleted'] == False]\n",
    "    df_maintenance = df_maintenance.drop_duplicates()\n",
    "    df_maintenance = df_maintenance[['id_ship', 'shipyard_code', 'start_docking_date',\n",
    "                                    'end_docking_date', 'actual_start_docking_date',\n",
    "                                    'actual_end_docking_date']]\n",
    "    return df_maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_tide():\n",
    "    sql = 'select * FROM rms_pelni.master_tide'\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    df_tide = df[df['is_deleted'] == False]\n",
    "    df_tide = df_tide[['id_port', 'start_date_time', 'end_date_time',\n",
    "                       'tide', 'min_tide', 'max_tide', 'avg_tide']]\n",
    "    df_tide = df_tide.rename(columns={'id_port' : 'idport', \n",
    "                                      'start_date_time' : 'start', \n",
    "                                      'end_date_time' : 'end', \n",
    "                                      'min_tide' : 'mintide', \n",
    "                                      'max_tide' : 'maxtide', \n",
    "                                      'avg_tide' : 'avgtide'})\n",
    "    df_tide['start'] = pd.to_datetime(df_tide['start'])\n",
    "    df_tide['end'] = pd.to_datetime(df_tide['end'])\n",
    "    \n",
    "    return df_tide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_priceconfig():\n",
    "    sql = 'select * FROM rms_pelni.price_config'\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    df_priceconfig = df[df['is_deleted'] == False]\n",
    "    df_priceconfig = df_priceconfig.drop_duplicates()\n",
    "    df_priceconfig = df_priceconfig[['min_distance', 'max_distance', 'coefficient',\n",
    "                                     'distance_coeff', 'pangsa']]\n",
    "    df_priceconfig = df_priceconfig.rename(columns={'min_distance' : 'mindistance', \n",
    "                                                    'max_distance' : 'maxdistance',\n",
    "                                                    'coefficient' : 'coeff', \n",
    "                                                    'distance_coeff' : 'distancecoeff'})\n",
    "    return df_priceconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_basefare():\n",
    "    sql = 'select * FROM rms_pelni.basefare_config'\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    df_basefare = df[df['is_deleted'] == False]\n",
    "    df_basefare = df_basefare[df_basefare['status_active'] == 'ACTIVE']\n",
    "    df_basefare = df_basefare.drop_duplicates()\n",
    "    df_basefare = df_basefare[['start_date', 'end_date', 'status_active',\n",
    "                               'type', 'base_fare', 'type_fare']]\n",
    "    df_basefare = df_basefare.rename(columns={'start_date' : 'startdate', \n",
    "                                              'end_date' : 'enddate',\n",
    "                                              'base_fare' : 'basefare',\n",
    "                                              'type_fare' : 'typefare'})\n",
    "    return df_basefare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_rulecost():\n",
    "    sql = 'select * FROM rms_pelni.rule_cost'\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    df_rulecost = df[df[\"is_deleted\"] == False]\n",
    "    df_rulecost = df_rulecost.drop_duplicates()\n",
    "    df_rulecost = df_rulecost[['id_rule_cost','name', 'pax', 'sailing_time', 'berthing_time', \n",
    "                               'local_time', 'id_port', 'id_ship', 'cost', 'expenses_day']]\n",
    "    df_rulecost = df_rulecost.rename(columns={'id_rule_cost':'idrulecost', 'pax' : 'ispax', \n",
    "                                              'sailing_time' : 'issailing',  \n",
    "                                              'berthing_time' : 'isberthing',\n",
    "                                              'local_time' : 'localtime',\n",
    "                                              'id_port' : 'idport', \n",
    "                                              'id_ship' : 'idship', \n",
    "                                              'expenses_day':'perday'})\n",
    "    return df_rulecost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_lowpeak():\n",
    "    sql = 'select * FROM rms_pelni.master_low_peak'\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    df_lowpeak = df[df['is_deleted'] ==  False]\n",
    "    df_lowpeak = df_lowpeak.drop_duplicates()\n",
    "    df_lowpeak = df_lowpeak[['name', 'type', 'start_date', 'end_date']]\n",
    "    df_lowpeak = df_lowpeak.rename(columns={'start_date' : 'startdate', \n",
    "                                            'end_date' : 'enddate'})\n",
    "    return df_lowpeak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_pricecargoconfig():\n",
    "    sql = 'select * FROM rms_pelni.price_cargo_config'\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    df_pricecargoconfig = df[df['is_deleted'] ==  False]\n",
    "    df_pricecargoconfig = df_pricecargoconfig.drop_duplicates()\n",
    "    df_pricecargoconfig = df_pricecargoconfig[['min_distance', 'max_distance',\n",
    "                                               'constant', 'distance_constant']]\n",
    "    df_pricecargoconfig = df_pricecargoconfig.rename(columns={'min_distance' : 'mindistance', \n",
    "                                                              'max_distance' : 'maxdistance',\n",
    "                                                              'constant' : 'coeff', \n",
    "                                                              'distance_constant' : 'distancecoeff'})\n",
    "    return df_pricecargoconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_adjustment():\n",
    "    sql = 'select * FROM rms_pelni.adjustment_config'\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    df_adjustment = df[df['is_deleted'] == False]\n",
    "    df_adjustment = df_adjustment.drop_duplicates()\n",
    "    \n",
    "    return df_adjustment.value.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_cargoflat():\n",
    "    sql = 'select * FROM rms_pelni.cargo_flat_config'\n",
    "    df = pd.read_sql(sql, engine)\n",
    "    df['is_deleted'] = df['is_deleted'].fillna(False)\n",
    "    \n",
    "    df_cargoflat = df[df['is_deleted'] == False]\n",
    "    df_cargoflat = df_cargoflat.drop_duplicates()\n",
    "    df_cargoflat = df_cargoflat[['port_origin', 'port_destination', 'type', 'fare']]\n",
    "    df_cargoflat = df_cargoflat.rename(columns={'port_origin' : 'id_origin',\n",
    "                                                'port_destination' : 'id_destination'})\n",
    "    return df_cargoflat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_rlsroute():\n",
    "    sql = 'select * FROM rms_pelni.rls_route'\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    df_rlsroute = df[df['is_deleted'] == False]\n",
    "    df_rlsroute = df_rlsroute[df_rlsroute['status'] == 'ACTIVE']\n",
    "    df_rlsroute = df_rlsroute.drop_duplicates()\n",
    "    df_rlsroute = df_rlsroute[['id_rls_route', 'avg_factor', 'avg_onboard', 'commision', 'tot_cost', 'tot_distance', \n",
    "                               'tot_revenue', 'tot_total', 'type_season', 'id_est_route', 'id_ship']]\n",
    "    df_rlsroute = df_rlsroute.rename(columns={'id_rls_route' : 'id', 'type_season' : 'season', 'id_est_route' : 'idest', \n",
    "                                              'id_ship' : 'idship'})\n",
    "    return df_rlsroute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_rlsroutedetail():\n",
    "    sql = 'select * FROM rms_pelni.rls_route_detail '\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    df_rlsroutedetail = df[df['is_deleted'] == False]\n",
    "    df_rlsroutedetail = df_rlsroutedetail.drop_duplicates()\n",
    "    df_rlsroutedetail = df_rlsroutedetail[['id_rls_route_detail', 'factor', 'rls_down', 'rls_onboard', 'rls_revenue', \n",
    "                                           'rls_total', 'rls_up', 'ruas', 'type', 'id_port_origin', 'id_rls_route']]\n",
    "    df_rlsroutedetail = df_rlsroutedetail.rename(columns={'id_rls_route_detail' : 'id', 'rls_down' : 'down', \n",
    "                                                          'rls_onboard' : 'onboard', 'rls_revenue' : 'revenue', \n",
    "                                                          'rls_total' : 'total', 'rls_up' : 'up', \n",
    "                                                          'id_port_origin' : 'idport', 'id_rls_route' : 'idrls'})\n",
    "    return df_rlsroutedetail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_routecost():\n",
    "    sql = 'select * FROM rms_pelni.rls_route_cost'\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    df_routecost = df[df['is_deleted'] == False]\n",
    "    df_routecost = df_routecost.drop_duplicates()\n",
    "    df_routecost = df_routecost[['id_rls_cost', 'berthing_time', 'npax', 'sailing_time', 'subtotal_cost']]\n",
    "    df_routecost = df_routecost.rename(columns={'id_rls_cost' : 'id'})\n",
    "    return df_routecost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_tide():\n",
    "    sql = 'select * FROM rms_pelni.master_tide'\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    df_tide = df[df['is_deleted'] == False]\n",
    "    df_tide = df_tide.drop_duplicates()\n",
    "    df_tide = df_tide[['id_tide', 'tide', 'start_date_time', 'end_date_time', 'id_port']]\n",
    "    df_tide = df_tide.rename(columns={'start_date_time' : 'start', 'end_date_time' : 'end'})\n",
    "    return df_tide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_revenue_period(typerev, date_start, date_end):\n",
    "    sql =  f\"select * from rms_pelni.hist_pax_revenue hpr where type_rev = {typerev} and departure_date between '{date_start}' and '{date_end}'\"\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    #df_revenue = df[df['type_rev'] == type_rev]\n",
    "    df_revenue = df[df['is_deleted'] == False]\n",
    "    df_revenue = df_revenue.drop_duplicates()\n",
    "    df_revenue = df_revenue[['origin_port', 'destination_port', 'departure_date',\n",
    "                             'departure', 'revenue', 'total', 'type']]\n",
    "    df_revenue = df_revenue.rename(columns={'origin_port' : 'origin',\n",
    "                                            'destination_port' : 'destination',\n",
    "                                            'departure_date' : 'depdate',\n",
    "                                            'departure' : 'deptime'})\n",
    "    return df_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_hist_pax(start,end):\n",
    "    hist_rev1 = retrieve_revenue_period(1,start,end)\n",
    "    hist_rev2 = retrieve_revenue_period(2,start,end)\n",
    "    merged_df = pd.merge(hist_rev1, hist_rev2, on=[\n",
    "                        'origin', 'destination', 'type'], how='outer', indicator=True)\n",
    "    \n",
    "    merged_df_right = merged_df[merged_df['_merge']=='right_only']\n",
    "    merged_df_right = merged_df_right.drop(columns=['depdate_x', 'deptime_x', 'revenue_x', 'total_x','_merge'])\n",
    "    merged_df_right.rename(columns={\n",
    "        'depdate_y':'depdate',\n",
    "        'deptime_y':'deptime',\n",
    "        'revenue_y':'revenue',\n",
    "        'total_y':'total'\n",
    "    }, inplace=True)\n",
    "   \n",
    "    merged_df_left = merged_df[merged_df['_merge'] == 'left_only']\n",
    "    merged_df_left = merged_df_left.drop(columns=['depdate_y', 'deptime_y', 'revenue_y', 'total_y','_merge'])\n",
    "    merged_df_left.rename(columns={\n",
    "        'depdate_x': 'depdate',\n",
    "        'deptime_x': 'deptime',\n",
    "        'revenue_x': 'revenue',\n",
    "        'total_x': 'total'\n",
    "    }, inplace=True)\n",
    "\n",
    "    merged_hist_pax = pd.merge(merged_df_left, merged_df_right, how='outer')\n",
    "\n",
    "    return merged_hist_pax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing_portdistance(df_port, df_portdistance) :\n",
    "    #global df_portdistance\n",
    "\n",
    "    # Add data dummy\n",
    "    # dum = copy.deepcopy(df_port[df_port['portno'] == 10])\n",
    "    # dum = dum.reset_index()\n",
    "    # dum.drop('portno', axis=1, inplace=True)\n",
    "\n",
    "    # # Update data dummy\n",
    "    # dum['id'], dum['portcode'], dum['name'], dum['code'], dum['lat'], dum['long'], dum['hightide'], dum['lowtide'], dum['length'] = '111', 'AZZZ', 'DUMMY', '99001', '-6.1312617', '106.6409313', '3.1', '4.1', '99' \n",
    "    # df_port = pd.concat([df_port, dum])\n",
    "\n",
    "    #START function\n",
    "    portlist = copy.deepcopy(list(df_port.id))\n",
    "    od_pair = pd.DataFrame((itertools.permutations(portlist, 2)), columns=['id_origin', 'id_destination'])\n",
    "\n",
    "    # Permutation (id_port)\n",
    "    df_join = pd.merge(df_portdistance, od_pair, how='right', left_on=('id_origin', 'id_destination'), right_on=('id_origin', 'id_destination'))\n",
    "    notfound = df_join[df_join['nautical'].isna()]\n",
    "    # notfound\n",
    "    df = notfound.drop(columns=['origin','destination'])\n",
    "    \n",
    "    df_ori = copy.deepcopy(df_port.loc[:, ['id','lat', 'long']])\n",
    "    nf = notfound.loc[:, ['id_origin','id_destination']]\n",
    "    df_des = copy.deepcopy(df_ori)\n",
    "\n",
    "    # Change name columns\n",
    "    df_ori = df_ori.rename(columns={'id': 'id_origin', 'lat': 'lat_origin', 'long': 'long_origin'})\n",
    "    df_des = df_des.rename(columns={'id': 'id_destination', 'lat': 'lat_destination', 'long': 'long_destination'})\n",
    "    \n",
    "    # Change Typedate\n",
    "    nf['id_origin'] = nf['id_origin'].astype(str)\n",
    "    df_ori['id_origin'] = df_ori['id_origin'].astype(str)\n",
    "\n",
    "    # Inner Join\n",
    "    df = pd.merge(nf, \n",
    "                df_ori, \n",
    "                on ='id_origin', \n",
    "                how ='inner')\n",
    "    \n",
    "    # Change Typedata\n",
    "    df['id_destination'] = df['id_destination'].astype(str)\n",
    "    df_des['id_destination'] = df_des['id_destination'].astype(str)\n",
    "\n",
    "    # Inner Join\n",
    "    df = pd.merge(df,\n",
    "                    df_des,\n",
    "                    on ='id_destination', \n",
    "                    how ='inner')\n",
    "\n",
    "    # Change Typedata\n",
    "    df['lat_origin'] = df['lat_origin'].astype(float)\n",
    "    df['long_origin'] = df['long_origin'].astype(float)\n",
    "    df['lat_destination'] = df['lat_destination'].astype(float)\n",
    "    df['long_destination'] = df['long_destination'].astype(float)\n",
    "\n",
    "    # Calculate from latitude and longtitude data convert to miles (use other library)\n",
    "    nautical_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        ori = (row['lat_origin'],row['long_origin'])\n",
    "        des = (row['lat_destination'],row['long_destination'])\n",
    "        rs = gd(ori, des).miles\n",
    "        rs = round(rs)\n",
    "        if rs==0:\n",
    "            rs = 99999\n",
    "        nautical_data.append(rs)\n",
    "        \n",
    "    # Add new columns    \n",
    "    df['nautical'] = nautical_data\n",
    "    df['commercial'] = nautical_data\n",
    "    df['origin'] = 99999\n",
    "    df['destination'] = 99999\n",
    "    if len(df)>0:\n",
    "        df_portdistance = pd.concat([df_portdistance,df[['origin','destination','nautical','commercial','id_origin','id_destination']]])\n",
    "    \n",
    "    return df_portdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cleaned_port(df_port):\n",
    "#     port = copy.deepcopy(df_port)\n",
    "    \n",
    "#     port['depth'] = port['depth'].astype(str).str.extract('(\\d+)')\n",
    "#     port.loc[((port['depth'] == '') | (port['depth'] == None) | (port['depth'].isna())), 'depth'] = 10\n",
    "#     port['depth'] = port['depth'].astype(float)\n",
    "\n",
    "#     port['length'] = port['length'].astype(str).str.extract('(\\d+)')\n",
    "#     port.loc[((port['length'] == '') | (port['length'] == None) | (port['length'].isna())), 'length'] = 200\n",
    "#     port['length'] = port['length'].astype(float)\n",
    "\n",
    "#     port['avgberth'] = port['avgberth'].astype(str).str.extract('(\\d+)').astype(float)\n",
    "#     port.loc[((port['avgberth'] == '') | (port['avgberth'] == None) | (port['avgberth'].isna())), 'avgberth'] = 2\n",
    "    \n",
    "#     port['lat'] = port['lat'].astype(str).str.extract('(\\d+)')\n",
    "#     port.loc[((port['lat'] == '') | (port['lat'] == None) | (port['lat'].isna())), 'lat'] = 24\n",
    "#     port['lat'] = port['lat'].astype(float)\n",
    "\n",
    "#     port['long'] = port['long'].astype(str).str.extract('(\\d+)')\n",
    "#     port.loc[((port['long'] == '') | (port['long'] == None) | (port['long'].isna())), 'long'] = (-96)\n",
    "#     port['long'] = port['long'].astype(float)\n",
    "    \n",
    "#     port['noberth'] = port['noberth'].astype(str).str.extract('(\\d+)')\n",
    "#     port.loc[((port['noberth'] == '') | (port['noberth'] == None) | (port['noberth'].isna())), 'noberth'] = 1\n",
    "#     port['noberth'] = port['noberth'].astype(int)\n",
    "\n",
    "#     for i,p in port.iterrows():\n",
    "#         port.loc[i,'minberthtime'] = str(port.loc[i,'minberthtime'])\n",
    "#         port.loc[i,'maxberthtime'] = str(port.loc[i,'maxberthtime'])\n",
    "#     minberthtime = '00:00:00'\n",
    "#     minberthtime = datetime.strptime(minberthtime, '%H:%M:%S')\n",
    "#     port.loc[((port['minberthtime'] == 0) | (port['minberthtime'] == 'None') | (port['minberthtime'].isna())), 'minberthtime'] = minberthtime\n",
    "#     port['minberthtime'] = pd.to_datetime(port['minberthtime'])\n",
    "#     port['minberthtime'] = port['minberthtime'].dt.time\n",
    "        \n",
    "#     maxberthtime = '23:59:59'\n",
    "#     maxberthtime = datetime.strptime(maxberthtime, '%H:%M:%S')\n",
    "#     port.loc[((port['maxberthtime'] == 0) | (port['maxberthtime'] == 'None') | (port['maxberthtime'].isna())), 'maxberthtime'] = maxberthtime\n",
    "#     port['maxberthtime'] = pd.to_datetime(port['maxberthtime'])\n",
    "#     port['maxberthtime'] = port['maxberthtime'].dt.time\n",
    "\n",
    "#     return port"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
